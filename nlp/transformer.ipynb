{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be33024c-2e5a-4ece-97d9-f2b1b3247dfd",
   "metadata": {},
   "source": [
    "# NN.TRANSFORMER로 언어 번역 (eng-fra.txt 사용)\n",
    "https://tutorials.pytorch.kr/beginner/translation_transformer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21d1ecca-c0e8-415e-b911-2333980a2b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import os\n",
    "import unicodedata\n",
    "import re\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d065d200-37fc-46ca-a740-68c970ed70fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "UNK, PAD, BOS, EOS = 0, 1, 2, 3\n",
    "\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"<unk>\", 1: \"<pad>\", 2: \"<bos>\", 3: \"<eos>\"}\n",
    "        self.n_words = 4  # SOS 와 EOS 포함\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9ed40a9a-21e8-4b11-a610-5476d14a5208",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s\n",
    "\n",
    "\n",
    "MAX_LENGTH = 30\n",
    "\n",
    "eng_prefixes = (\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is\", \"he s \",\n",
    "    \"she is\", \"she s \",\n",
    "    \"you are\", \"you re \",\n",
    "    \"we are\", \"we re \",\n",
    "    \"they are\", \"they re \"\n",
    ")\n",
    "\n",
    "\n",
    "def filterPair(p):\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
    "        len(p[1].split(' ')) < MAX_LENGTH and \\\n",
    "        p[1].startswith(eng_prefixes)\n",
    "\n",
    "\n",
    "def readLangs(lang1, lang2, reverse=False):\n",
    "    print(\"Reading lines...\")\n",
    "\n",
    "    # 파일을 읽고 줄로 분리\n",
    "    lines = open('../data/%s-%s.txt' % (lang1, lang2), encoding='utf-8').\\\n",
    "        read().strip().split('\\n')\n",
    "\n",
    "    # 모든 줄을 쌍으로 분리하고 정규화\n",
    "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
    "\n",
    "    # 쌍을 뒤집고, Lang 인스턴스 생성\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]\n",
    "\n",
    "\n",
    "def prepareData(lang1, lang2, reverse=False):\n",
    "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    pairs = filterPairs(pairs)\n",
    "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "\n",
    "def to_vec(lang, s, max_len):\n",
    "    vec = [BOS] + [lang.word2index[word] for word in s.split()] + [EOS]\n",
    "    vec += [PAD] * (max_len - len(vec))\n",
    "    return torch.tensor(vec).view(-1, 1)\n",
    "\n",
    "\n",
    "def to_sentence(lang, vec):\n",
    "    return \" \".join(lang.index2word[i.item()] for i in vec)\n",
    "\n",
    "\n",
    "def src_tgt_pair(input_lang, output_lang, pair, src_len=32, tgt_len=32):\n",
    "    i, o = pair\n",
    "    return to_vec(input_lang, i, src_len), to_vec(output_lang, o, tgt_len)\n",
    "\n",
    "\n",
    "def get_batch(input_lang, output_lang, pairs, batch_size=64):\n",
    "    src, tgt = [], []\n",
    "    for pair in random.choices(pairs, k=batch_size):\n",
    "        s, t = src_tgt_pair(input_lang, output_lang, pair)\n",
    "        src.append(s)\n",
    "        tgt.append(t)\n",
    "    return torch.cat(src, 1), torch.cat(tgt, 1)\n",
    "\n",
    "\n",
    "def generate_square_subsequent_mask(sz):\n",
    "    mask = torch.triu(torch.ones(sz, sz), diagonal=1)\n",
    "    mask[torch.where(mask == 1)] = float(\"-inf\")\n",
    "    return mask\n",
    "\n",
    "\n",
    "def create_mask(src, tgt):\n",
    "    src_seq_len = src.shape[0]\n",
    "    tgt_seq_len = tgt.shape[0]\n",
    "\n",
    "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n",
    "    src_mask = torch.zeros(src_seq_len, src_seq_len).type(torch.bool)\n",
    "\n",
    "    src_padding_mask = (src == PAD).transpose(0, 1)\n",
    "    tgt_padding_mask = (tgt == PAD).transpose(0, 1)\n",
    "    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35944ed9-baa9-4741-a535-33304afe9324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 135842 sentence pairs\n",
      "Trimmed to 13067 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "fra 5173\n",
      "eng 3391\n",
      "['je pense cloturer mon compte d epargne .', 'i am thinking of closing my savings account .']\n"
     ]
    }
   ],
   "source": [
    "input_lang, output_lang, pairs = prepareData('eng', 'fra', True)\n",
    "print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3ef4cae-8b9a-4225-b0f6-0549b4d73480",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self,\n",
    "                 emb_size,\n",
    "                 dropout,\n",
    "                 maxlen):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n",
    "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
    "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
    "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
    "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
    "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('pos_embedding', pos_embedding)\n",
    "\n",
    "    def forward(self, token_embedding):\n",
    "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n",
    "\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
    "        self.emb_size = emb_size\n",
    "\n",
    "    def forward(self, tokens):\n",
    "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n",
    "\n",
    "class NN(nn.Module):\n",
    "    def __init__(self, n_src_vocab, n_tgt_vocab, n_emb, dropout=0.1, max_seq_size=32):\n",
    "        super(NN, self).__init__()\n",
    "        self.src_emb = TokenEmbedding(n_src_vocab, n_emb)\n",
    "        self.tgt_emb = TokenEmbedding(n_tgt_vocab, n_emb)\n",
    "        self.pos_enc = PositionalEncoding(n_emb, dropout, max_seq_size)\n",
    "        self.transformer = nn.Transformer(d_model=n_emb, nhead=4, num_encoder_layers=2, num_decoder_layers=2, dim_feedforward=512)\n",
    "        self.generator = nn.Linear(n_emb, n_tgt_vocab)\n",
    "        \n",
    "    def forward(self, src, tgt, src_mask, tgt_mask, src_pad_mask, tgt_pad_mask, memory_key_pad_mask):\n",
    "        src_emb = self.pos_enc(self.src_emb(src))\n",
    "        tgt_emb = self.pos_enc(self.tgt_emb(tgt))\n",
    "        out = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None, src_pad_mask, tgt_pad_mask, memory_key_pad_mask)\n",
    "        return self.generator(out)\n",
    "    \n",
    "    def encode(self, src, src_mask):\n",
    "        return self.transformer.encoder(\n",
    "            self.pos_enc(self.src_emb(src)), src_mask)\n",
    "\n",
    "    def decode(self, tgt, memory, tgt_mask):\n",
    "        return self.transformer.decoder(\n",
    "            self.pos_enc(self.tgt_emb(tgt)), memory, tgt_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74e40220-f3e2-47a3-8e95-dbc4c77673ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5173, 3391)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_src_vocab = input_lang.n_words\n",
    "n_tgt_vocab = output_lang.n_words\n",
    "n_emb = 128\n",
    "model = NN(n_src_vocab, n_tgt_vocab, n_emb)\n",
    "\n",
    "n_src_vocab, n_tgt_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20b3a7ec-44d2-443a-9bbc-5b7288acc4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 2\n",
    "n_itr = 100\n",
    "b_size = 64\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=PAD)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1aee8084-f346-4bb9-b4cb-d02893e054ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:36<00:00,  2.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0 36.848s loss - 4.9024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:37<00:00,  2.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1 74.755s loss - 3.9639\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "s_time = time.time()\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    total_loss = 0\n",
    "    for iter in tqdm(range(n_itr)):\n",
    "        src, tgt = get_batch(input_lang, output_lang, pairs, b_size)\n",
    "        tgt_in = tgt[:-1, :]\n",
    "        src_mask, tgt_mask, src_pad_mask, tgt_pad_mask = create_mask(src, tgt_in)\n",
    "        tgt_out = tgt[1:, :]\n",
    "        optimizer.zero_grad()\n",
    "        out = model(src, tgt_in, src_mask, tgt_mask, src_pad_mask, tgt_pad_mask, src_pad_mask)\n",
    "        loss = loss_fn(out.reshape(-1, out.shape[-1]), tgt_out.reshape(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss\n",
    "    torch.save(model.state_dict(), \"transformer.pth\")\n",
    "    print(\"\\n%d %.3fs loss - %.4f\" % (epoch, time.time() - s_time, total_loss / n_itr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0c93819a-7e79-41f5-a9b4-c814475cae6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
    "    memory = model.encode(src, src_mask)\n",
    "    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long)\n",
    "    for i in range(max_len-1):\n",
    "        tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n",
    "                    .type(torch.bool))\n",
    "        out = model.decode(ys, memory, tgt_mask)\n",
    "        out = out.transpose(0, 1)\n",
    "        prob = model.generator(out[:, -1])\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        next_word = next_word.item()\n",
    "\n",
    "        ys = torch.cat([ys,\n",
    "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n",
    "        if next_word == EOS:\n",
    "            break\n",
    "    return ys\n",
    "\n",
    "\n",
    "# 입력 문장을 도착어로 번역하는 함수\n",
    "def translate(model, input_lang, output_lang, src_sentence):\n",
    "    src_sentence = normalizeString(src_sentence)\n",
    "    src = to_vec(input_lang, src_sentence, 32)\n",
    "    num_tokens = src.shape[0]\n",
    "    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
    "    tgt_tokens = greedy_decode(\n",
    "        model, src, src_mask, max_len=num_tokens + 5, start_symbol=BOS).flatten()\n",
    "    return to_sentence(output_lang, tgt_tokens).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8ba3c3c4-df23-439e-b1f8-5ce6b5d20a43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' i m not . '"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(model, input_lang, output_lang, \"Je ne m'en vais pas .\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9b4872-f973-456b-812b-8a51127a2292",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
